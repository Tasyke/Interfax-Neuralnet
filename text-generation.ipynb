{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "text-generation.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdMp7KFdDz7E"
      },
      "source": [
        "# Char-based text generation with LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvuZ_CZzDz7I"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdwJoOvDDz7O",
        "outputId": "c2b418c8-1f46-4497-caf0-122c903ac97a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "TRAIN_TEXT_FILE_PATH = 'data.txt'\n",
        "\n",
        "with open(TRAIN_TEXT_FILE_PATH) as text_file:\n",
        "    text_sample = text_file.readlines()\n",
        "text_sample = ' '.join(text_sample)\n",
        "\n",
        "def text_to_seq(text_sample):\n",
        "    char_counts = Counter(text_sample)\n",
        "    char_counts = sorted(char_counts.items(), key = lambda x: x[1], reverse=True)\n",
        "\n",
        "    sorted_chars = [char for char, _ in char_counts]\n",
        "    print(sorted_chars)\n",
        "    char_to_idx = {char: index for index, char in enumerate(sorted_chars)}\n",
        "    idx_to_char = {v: k for k, v in char_to_idx.items()}\n",
        "    sequence = np.array([char_to_idx[char] for char in text_sample])\n",
        "    \n",
        "    return sequence, char_to_idx, idx_to_char\n",
        "\n",
        "sequence, char_to_idx, idx_to_char = text_to_seq(text_sample)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[' ', 'о', 'а', 'и', 'е', 'н', 'р', 'т', 'с', 'в', 'С', 'И', 'Р', 'О', 'А', 'л', 'к', '\\n', 'Н', 'Е', 'п', 'д', 'К', 'у', 'Т', 'м', 'ы', 'В', 'з', 'М', 'я', 'Я', 'б', 'г', 'П', 'Л', 'й', 'Д', 'Б', '1', 'Г', 'ь', '0', 'З', 'ч', 'У', 'Ы', '2', 'ж', 'ц', 'х', 'Ц', '\"', ',', 'Ф', 'ю', 'ш', 'ф', ':', 'Ш', 'Э', '3', '5', 'Ь', '9', 'D', 'Ж', '.', 'Ч', 'R', 'Й', 'O', 'I', '4', 'щ', 'A', 'Ю', 'э', '7', 'C', 'e', 'V', 'a', 'Х', '6', '8', 'n', '+', 'r', 'T', '%', 't', '/', 'E', '(', ')', 's', 'S', 'o', 'i', 'G', 'Щ', '$', 'c', 'M', 'N', 'Z', 'ъ', 'U', 'W', 'X', 'P', 'B', 'p', 'l', 'k', 'Ъ', 'd', 'J', 'w', 'F', 'f', 'z', 'h', 'u', '*', 'b', 'm', 'v', 'ё', '&', 'L', 'H', 'Y', 'K', ';', 'g', 'x', 'y', 'Q']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JhURBmoDz7U"
      },
      "source": [
        "SEQ_LEN = 256\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "def get_batch(sequence):\n",
        "    trains = []\n",
        "    targets = []\n",
        "    for _ in range(BATCH_SIZE):\n",
        "        batch_start = np.random.randint(0, len(sequence) - SEQ_LEN)\n",
        "        chunk = sequence[batch_start: batch_start + SEQ_LEN]\n",
        "        train = torch.LongTensor(chunk[:-1]).view(-1, 1)\n",
        "        target = torch.LongTensor(chunk[1:]).view(-1, 1)\n",
        "        trains.append(train)\n",
        "        targets.append(target)\n",
        "    return torch.stack(trains, dim=0), torch.stack(targets, dim=0)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_4usSciDz7a"
      },
      "source": [
        "def evaluate(model, char_to_idx, idx_to_char, start_text=' ', prediction_len=200, temp=0.3):\n",
        "    hidden = model.init_hidden()\n",
        "    idx_input = [char_to_idx[char] for char in start_text]\n",
        "    train = torch.LongTensor(idx_input).view(-1, 1, 1).to(device)\n",
        "    predicted_text = start_text\n",
        "    \n",
        "    _, hidden = model(train, hidden)\n",
        "        \n",
        "    inp = train[-1].view(-1, 1, 1)\n",
        "    \n",
        "    for i in range(prediction_len):\n",
        "        output, hidden = model(inp.to(device), hidden)\n",
        "        output_logits = output.cpu().data.view(-1)\n",
        "        p_next = F.softmax(output_logits / temp, dim=-1).detach().cpu().data.numpy()        \n",
        "        top_index = np.random.choice(len(char_to_idx), p=p_next)\n",
        "        inp = torch.LongTensor([top_index]).view(-1, 1, 1).to(device)\n",
        "        predicted_char = idx_to_char[top_index]\n",
        "        predicted_text += predicted_char\n",
        "    \n",
        "    return predicted_text"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-uM-UE3Dz7d"
      },
      "source": [
        "class TextRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, embedding_size, n_layers=1):\n",
        "        super(TextRNN, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.encoder = nn.Embedding(self.input_size, self.embedding_size)\n",
        "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, self.n_layers)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc = nn.Linear(self.hidden_size, self.input_size)\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        x = self.encoder(x).squeeze(2)\n",
        "        out, (ht1, ct1) = self.lstm(x, hidden)\n",
        "        out = self.dropout(out)\n",
        "        x = self.fc(out)\n",
        "        return x, (ht1, ct1)\n",
        "    \n",
        "    def init_hidden(self, batch_size=1):\n",
        "        return (torch.zeros(self.n_layers, batch_size, self.hidden_size, requires_grad=True).to(device),\n",
        "               torch.zeros(self.n_layers, batch_size, self.hidden_size, requires_grad=True).to(device))"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-wYyPEgDz7o",
        "outputId": "3701135b-0730-4615-b80a-c07787ee555b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = TextRNN(input_size=len(idx_to_char), hidden_size=128, embedding_size=128, n_layers=2)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, amsgrad=True)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, \n",
        "    patience=5, \n",
        "    verbose=True, \n",
        "    factor=0.5\n",
        ")\n",
        "\n",
        "n_epochs = 50000\n",
        "loss_avg = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    train, target = get_batch(sequence)\n",
        "    train = train.permute(1, 0, 2).to(device)\n",
        "    target = target.permute(1, 0, 2).to(device)\n",
        "    hidden = model.init_hidden(BATCH_SIZE)\n",
        "\n",
        "    output, hidden = model(train, hidden)\n",
        "    loss = criterion(output.permute(1, 2, 0), target.squeeze(-1).permute(1, 0))\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    loss_avg.append(loss.item())\n",
        "    if len(loss_avg) >= 50:\n",
        "        mean_loss = np.mean(loss_avg)\n",
        "        print(f'Loss: {mean_loss}')\n",
        "        scheduler.step(mean_loss)\n",
        "        loss_avg = []\n",
        "        model.eval()\n",
        "        predicted_text = evaluate(model, char_to_idx, idx_to_char)\n",
        "        print(predicted_text)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 3.0703287172317504\n",
            " Сраде постостовит на 19 на постов в Моровити довния постов на рабали на простов прали на 19 на о повосстов остовна стодов остов на по в Модоля прастов пов пов постов на посков серания на посков о поли\n",
            "Loss: 2.2747684001922606\n",
            "  Моски в Моствертих на 13:00 МСК\n",
            " МИР ВАКЦИНА ПРОСТИКИ 11:00\n",
            " Резалинии на соблавении на сереновении в Моства в России в Моствании в Москов Московский остовном в Московский присли в РФ и США на 10 млн\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-184f18a9bdfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5Np10FxDz7y",
        "outputId": "6b1145c8-42de-44c4-de97-cbd15eb35fad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.eval()\n",
        "\n",
        "print(evaluate(\n",
        "    model, \n",
        "    char_to_idx, \n",
        "    idx_to_char, \n",
        "    temp=0.3, \n",
        "    prediction_len=1000, \n",
        "    start_text='. '\n",
        "    )\n",
        ")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".  Моставил провых в странии странии \"Фодольски в резовных продольской облении принитать по по по поления странии   Московской в РФ с в странии   Москов от потракцины острании   Мостарта по пристования поставии остовов акциями проводольской по сотратов РФ на по по пособлания проводольской по провых придет атала пристании с странии на постами в Москов в РФ притиков провых рестования простании проста на пристования простании в Предитать простова в РФ в РФ на простании по по по 11:01:01:01\n",
            " РОССИЯ ВАКЦИНА ПРОСС ВАКЦИНА ВАКЦИНА ПРОСС ПРЕДОНОВНОЙ РЫНОЗ ПРОСС ВАКЦИНА ПРОСС КРОСС ПРИГОСТАН ПРОЕНИЕ ПРОСС КРОСС РОССИЯ КРАСТА ПРОСС СТА ПРЕНИРЫ КРОСС ПРОСС КРОСС ИНДЕРОВОРС ПРОИЗВОСТАВИР ВАЗЫТИЯ ПРЕНИРЫ КОНОВНОЙ РЫНОЗ ПРОСС КРОСС КРОСС КУРСЫ\n",
            " Кросс проссии с пригание в Россия прогования продовния пристание проговать сатала   США по облистов на по проблинина по политать проиновном проводольской проставие продоблина по го посновных на поления прододоблания Astraeeaeca в по страстов в РФ раза США от ук\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}